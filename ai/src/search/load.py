# -*- coding: utf-8 -*-
"""Ai Search Kabeers Papers.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZLKp4WyPuxAUMjKYs2kWEH-TXOSccLdP

# AI Search Engine for OneSylabus

[1.1] Bert and ANN Index Model
"""
import json
import os
from pathlib import Path

from .encode import init_index, encode, encode_one, create_index, find
from .globals import ann_path, documents_cache_path, remote_documents_cache_path, remote_ann_path
from pymongo import MongoClient
from bson import json_util
import boto3

s3_bucket = boto3.resource('s3', aws_access_key_id='8E3B5E5290B83DEA900C',
                           aws_secret_access_key='Qt0l1JtIaA2UK5LC6Z9USYsvcPVmD4mOXPCtN4VY', region_name='us-east-1',
                           endpoint_url="https://kabeers-papers.s3.filebase.com").Bucket('kabeers-papers')
base = 'https://kabeers-papers-pdf2image.000webhostapp.com/ai'
client = MongoClient("mongodb://localhost:27017")
db = client.KabeersPastPapers


def _get_documents():
    print("finding docs")
    documents = list(db["document-pages"].find({}, {'_id': False}))
    documents = [{**doc, '_id': None} for doc in documents]
    mapping = dict(enumerate([doc['id'] for doc in documents]))
    return {'documents': documents, 'idmap': mapping}


def _file_exists(bucket, file):
    try:
        return not not bucket.Object(file).get()
    except:
        return False


def load_documents(bucket):
    if Path(documents_cache_path).exists():
        with open(documents_cache_path) as f:
            d = json.loads(f.read())
            documents = d['documents']
            idmap = d['idmap']
    elif _file_exists(bucket, remote_documents_cache_path):
        d = json.loads(bucket.Object(remote_documents_cache_path).get()['Body'].read().decode('utf-8'))
        documents = d['documents']
        idmap = d['idmap']
    else:
        d = _get_documents()
        documents = d['documents']
        idmap = d['idmap']
        with open(documents_cache_path, 'w') as f:
            json.dump({
                'idmap': idmap,
                'documents': documents
            }, f)
        bucket.Object(remote_documents_cache_path).put(Body=json.dumps({
            'idmap': idmap,
            'documents': documents
        }))
    return {
        'idmap': idmap,
        'documents': documents,
    }


def _index(idmap, documents, bucket):
    ann = init_index()
    print("encoding")
    encoded_documents = encode(documents)
    print("encoded")
    print(len(encoded_documents), idmap.get(documents[0]['id']))
    ann = create_index(ann, idmap, encoded_documents)
    save_index(bucket, ann)
    return ann


def index(idmap, documents, bucket):
    ann = None
    if Path(documents_cache_path).exists():
        ann = init_index(ann_path)
    elif _file_exists(bucket, remote_ann_path):
        bucket.download_file(remote_ann_path, ann_path)
        ann = init_index(ann_path)
    else:
        ann = init_index()
        encoded_documents = encode(documents)
        ann = create_index(ann, idmap, encoded_documents)
        save_index(bucket, ann)

    return ann


def save_index(bucket, ann):
    ann.save(ann_path)
    bucket.upload_file(ann_path, remote_ann_path)  # Remote
    # os.remove("./savedata/index.ann")
